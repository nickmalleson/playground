{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# (Old) tests of LLM tools\n",
    "\n",
    "Uses Huggingface Transformers to run the LLMs locally.\n",
    "\n",
    "_Not sure this is useful any more as I've mostly implemented this stuff properly elsewhere now_."
   ],
   "id": "78bfb1153bf1f15d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test 1\n",
    "\n",
    "\n",
    "Basic prompting using `Transformers`"
   ],
   "id": "4ced606544b5dea1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "#device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=True)\n",
    "model.to(device)\n",
    "\n",
    "# Define a function to create the prompt with examples\n",
    "def create_prompt(text):\n",
    "    \"\"\"\n",
    "    Create a prompt string with examples for text classification.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to classify.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete prompt with the provided text.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "Classify the following text as positive or negative sentiment:\n",
    "\n",
    "Text: \"I had a wonderful experience with this product. Highly recommend it!\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Text: \"The product broke the first time I used it. Very disappointing.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Text: \"I absolutely love this! Will buy again.\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Text: \"I hate this product. It did not meet my expectations at all.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Text: \"{text}\"\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_prompt2(text):\n",
    "    return f\"\"\"\n",
    "Classify the following text as positive or negative sentiment:\n",
    "Text: \"{text}\"\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "# Define a function to classify text using the model\n",
    "def classify_text(text, prompt_f):\n",
    "    \"\"\"\n",
    "    Classify the sentiment of the provided text using the LLaMA model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to classify.\n",
    "\n",
    "    Returns:\n",
    "        str: The classified sentiment (Positive or Negative).\n",
    "    \"\"\"\n",
    "    # Create the prompt with the input text\n",
    "    prompt = prompt_f(text)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Move input tensors to the MPS device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Generate a response from the model\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=inputs['input_ids'].shape[1] + 20, do_sample=False)\n",
    "\n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the sentiment from the generated response\n",
    "    sentiment = response.split(\"Sentiment:\")[-1].strip().split(\"\\n\")[0]\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for f in [create_prompt, create_prompt2]:\n",
    "    print(f\"Using prompt function: {f.__name__}\")\n",
    "\n",
    "    text = \"The movie was fantastic and I enjoyed every minute of it.\"\n",
    "    sentiment = classify_text(text, prompt_f=f)\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\")\n",
    "\n",
    "    text = \"What an atrocious display.\"\n",
    "    sentiment = classify_text(text, prompt_f=f)\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment}\")"
   ],
   "id": "239434c5cda9f088"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test 2",
   "id": "bb37a3ef724ee000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(prompt, model, tokenizer, max_length=512, temperature=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_length=max_length, temperature=temperature, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode and return the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = generate_response(prompt, model, tokenizer)\n",
    "print(f\"Prompt: {prompt}\\nResponse: {response}\")"
   ],
   "id": "dc68e10f90676f78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test 3\n",
    "\n",
    "Uses a pipeline"
   ],
   "id": "6da2a10e1d1a0f1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text generation pipeline with the specified model\n",
    "text_generation_pipeline = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(prompt, pipeline, max_length=512, temperature=0.7):\n",
    "    response = pipeline(prompt, max_length=max_length, temperature=temperature, pad_token_id=pipeline.tokenizer.eos_token_id)\n",
    "    return response[0]['generated_text']\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = generate_response(prompt, text_generation_pipeline)\n",
    "print(f\"Prompt: {prompt}\\nResponse: {response}\")"
   ],
   "id": "258c1b485e0fef16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Test 5\n",
    "\n",
    "Experiements with quantization."
   ],
   "id": "e4f0d370c11fa9b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "try:\n",
    "    torch.backends.quantized.engine = 'qnnpack'  # or 'fbgemm' for some setups (for quantization on the cpu)\n",
    "except Exception as e:  # Above doesn't work on my windows pc\n",
    "    print(e, \"using fbgemm instead\")\n",
    "    torch.backends.quantized.engine = 'fbgemm' # for some setups (for quantization on the cpu)\n",
    "\n",
    "# Paths for saving and loading the quantized model\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "quantized_model_path = \"quantized_model.pt\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to save the quantized model\n",
    "def save_quantized_model(model, path):\n",
    "    torch.save(model, path)\n",
    "\n",
    "# Function to load the quantized model\n",
    "def load_quantized_model(path):\n",
    "    return torch.load(path)\n",
    "\n",
    "# Check if quantized model exists\n",
    "if os.path.exists(quantized_model_path):\n",
    "    print(\"Loading quantized model from disk...\\n\")\n",
    "    quantized_model = load_quantized_model(quantized_model_path)\n",
    "else:\n",
    "    # Load and quantize the model\n",
    "    print(\"Loading model...\\n\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(\"Quantizing model...\\n\")\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    # Save the quantized model to disk\n",
    "    print(\"Saving quantized model...\\n\")\n",
    "    save_quantized_model(quantized_model, quantized_model_path)\n",
    "    # Free up memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Create the text generation pipeline with the quantized model\n",
    "print(\"Creating text generation pipeline...\\n\")\n",
    "text_generation_pipeline = pipeline(\"text-generation\",\n",
    "                                    model=quantized_model,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    device=\"cpu\",\n",
    "                                    max_new_tokens=10  # Cut off the output as we just want one word\n",
    "                                    )\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(prompt, pipeline, temperature=0.7):\n",
    "    full_response = pipeline(prompt, temperature=temperature, pad_token_id=pipeline.tokenizer.eos_token_id)\n",
    "    generated_text = full_response[0]['generated_text']\n",
    "    # The generated text includes the prmopt. Extract the sentiment from the generated text by splitting on the prompt\n",
    "    sentiment = generated_text.split(\"Sentiment (one word):\")[-1].strip().split()[0].lower()\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "def create_prompt2(text, show_working=False):\n",
    "    return f\"\"\"\n",
    "Classify the following text as positive, neutral or negative sentiment{'' if not show_working else ', showing your working'}:\n",
    "Text: \"{text}\"\n",
    "Sentiment (one word):\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt3(text):\n",
    "    return f\"\"\"\n",
    "Classify the following text as \"positive\", \"neutral\", or \"negative\" sentiment:\n",
    "\n",
    "Text: \"I love sunny days!\"\n",
    "Sentiment: positive\n",
    "\n",
    "Text: \"It's an okay day.\"\n",
    "Sentiment: neutral\n",
    "\n",
    "Text: \"I am not happy with the service.\"\n",
    "Sentiment: negative\n",
    "\n",
    "Text: \"{text}\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Example usage\n",
    "texts = [\n",
    "    \"The movie was fantastic and I enjoyed every minute of it.\",\n",
    "    \"What an atrocious display.\",\n",
    "    \"Hollis' death scene will hurt me severely to watch on film wry is directors cut not out now?\",\n",
    "    \"@smarrison i would've been the first, but i didn't have a gun. not really though, zac snyder's ju...\",\n",
    "    \"about to file taxes \",\n",
    "    \"im sad now Miss.Lilly\",\n",
    "    \"ooooh.... LOL that leslie.... and ok I won't do it again so leslie won't get mad again \",\n",
    "    \"Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my old life\",\n",
    "    \"Sad, sad, sad. I don't know why but I hate this feeling I wanna sleep and I still can't!\",\n",
    "    \"great day!\"\n",
    "]\n",
    "\n",
    "print(\"Beginning text generation...\\n\")\n",
    "for i, text in enumerate(texts):\n",
    "    start_time = datetime.now()\n",
    "    prompt = create_prompt2(text)\n",
    "    response = generate_response(prompt, text_generation_pipeline)\n",
    "    end_time = datetime.now() - start_time\n",
    "    print(f\"{i} Time taken: {end_time}.\\n\\tText: {text}\\n\\tPrompt: {prompt}\\n\\tResponse: {response}\")"
   ],
   "id": "87db04bb01a17713"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "85c611c902aded7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2cc56da5b365e361"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "93752f9bd7342cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
